{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10QvLz-gCqTCqQu5-ePXNjBJ3aogTxzhp",
      "authorship_tag": "ABX9TyOFiWJW6i97hEQcWteyMHXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mistryvivek/YRKCS-PRBX/blob/main/Model_1a_Basic_NN_(Earth_movers_distance).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BKFIobTm36L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOupk8NfiXR7",
        "outputId": "c565994e-8aed-4ecb-d77c-bb8e5eeaa130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Weights and Bias.\n",
        "\n",
        "* AI experimental tool."
      ],
      "metadata": {
        "id": "cnr6oqBffyAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "cA-o8vzff3Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "3F4388sHf7FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "0KY69sdrgAqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "a446dd87-765f-4cbe-ec80-7d7c5587a826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Earth Movers Distance Loss\n",
        "\n",
        "* https://github.com/TakaraResearch/Pytorch-1D-Wasserstein-Statistical-Loss/blob/master/pytorch_stats_loss.py"
      ],
      "metadata": {
        "id": "dAdUs-wdrOjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "#######################################################\n",
        "#       STATISTICAL DISTANCES(LOSSES) IN PYTORCH      #\n",
        "#######################################################\n",
        "\n",
        "## Statistial Distances for 1D weight distributions\n",
        "## Inspired by Scipy.Stats Statistial Distances for 1D\n",
        "## Pytorch Version, supporting Autograd to make a valid Loss\n",
        "## Supposing Inputs are Groups of Same-Length Weight Vectors\n",
        "## Instead of (Points, Weight), full-length Weight Vectors are taken as Inputs\n",
        "## Code Written by E.Bao, CASIA\n",
        "\n",
        "def torch_wasserstein_loss(tensor_a,tensor_b):\n",
        "    #Compute the first Wasserstein distance between two 1D distributions.\n",
        "    return(torch_cdf_loss(tensor_a,tensor_b,p=1))\n",
        "\n",
        "def torch_energy_loss(tensor_a,tensor_b):\n",
        "    # Compute the energy distance between two 1D distributions.\n",
        "    return((2**0.5)*torch_cdf_loss(tensor_a,tensor_b,p=2))\n",
        "\n",
        "def torch_cdf_loss(tensor_a,tensor_b,p=1):\n",
        "    # last-dimension is weight distribution\n",
        "    # p is the norm of the distance, p=1 --> First Wasserstein Distance\n",
        "    # to get a positive weight with our normalized distribution\n",
        "    # we recommend combining this loss with other difference-based losses like L1\n",
        "\n",
        "    # normalize distribution, add 1e-14 to divisor to avoid 0/0\n",
        "    tensor_a = tensor_a / (torch.sum(tensor_a, dim=-1, keepdim=True) + 1e-14)\n",
        "    tensor_b = tensor_b / (torch.sum(tensor_b, dim=-1, keepdim=True) + 1e-14)\n",
        "    # make cdf with cumsum\n",
        "    cdf_tensor_a = torch.cumsum(tensor_a,dim=-1)\n",
        "    cdf_tensor_b = torch.cumsum(tensor_b,dim=-1)\n",
        "\n",
        "    # choose different formulas for different norm situations\n",
        "    if p == 1:\n",
        "        cdf_distance = torch.sum(torch.abs((cdf_tensor_a-cdf_tensor_b)),dim=-1)\n",
        "    elif p == 2:\n",
        "        cdf_distance = torch.sqrt(torch.sum(torch.pow((cdf_tensor_a-cdf_tensor_b),2),dim=-1))\n",
        "    else:\n",
        "        cdf_distance = torch.pow(torch.sum(torch.pow(torch.abs(cdf_tensor_a-cdf_tensor_b),p),dim=-1),1/p)\n",
        "\n",
        "    cdf_loss = cdf_distance.mean()\n",
        "    return cdf_loss\n",
        "\n",
        "def torch_validate_distibution(tensor_a,tensor_b):\n",
        "    # Zero sized dimension is not supported by pytorch, we suppose there is no empty inputs\n",
        "    # Weights should be non-negetive, and with a positive and finite sum\n",
        "    # We suppose all conditions will be corrected by network training\n",
        "    # We only check the match of the size here\n",
        "    if tensor_a.size() != tensor_b.size():\n",
        "        raise ValueError(\"Input weight tensors must be of the same size\")"
      ],
      "metadata": {
        "id": "cFslJkAYrQqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify code for sliding window approach (like wk11 code).\n",
        "\n",
        "* Look at the full race image instead.\n",
        "* What data types are in the wk11 practical.\n",
        "* No vocab size required because they apply to catogorical contexts."
      ],
      "metadata": {
        "id": "0vxpMwPmrCNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = r\"/content/drive/MyDrive/prbx_data/v1/\"\n",
        "max_race_size = 0\n",
        "max_tyre_life = 0"
      ],
      "metadata": {
        "id": "VEYJqy9UAizz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "training_inputs = []\n",
        "training_outputs = []\n",
        "\n",
        "RaceCalender22 = pd.read_csv(BASE_PATH + r\"2022/eventCalender2022.csv\")\n",
        "for _, row in RaceCalender22.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2022/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            if max(TempRaceLoadDriver['TyreLife']) > max_tyre_life:\n",
        "                max_tyre_life = max(TempRaceLoadDriver['TyreLife'])\n",
        "\n",
        "            stint_change_array = [0 if stint_array[i] == stint_array[i + 1]else 1\n",
        "                                  for i in range(len(stint_array) - 1)] + [0]\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "\n",
        "            training_inputs.append(tyre_life_array)\n",
        "            training_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "muIWawoGrFrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "testing_inputs = []\n",
        "testing_outputs = []\n",
        "\n",
        "RaceCalender23 = pd.read_csv(BASE_PATH + r\"2023/eventCalender2023.csv\")\n",
        "for _, row in RaceCalender23.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2023/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            if max(TempRaceLoadDriver['TyreLife']) > max_tyre_life:\n",
        "                max_tyre_life = max(TempRaceLoadDriver['TyreLife'])\n",
        "\n",
        "            stint_change_array = [0 if stint_array[i] == stint_array[i + 1]else 1\n",
        "                                  for i in range(len(stint_array) - 1)] + [0]\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "\n",
        "            testing_inputs.append(tyre_life_array)\n",
        "            testing_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "fEz0-Y5DrrEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNv1b(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size): #vocab_size\n",
        "        super(RNNv1b, self).__init__()\n",
        "        #self.embedding =  nn.Embedding(vocab_size, hidden_size)\n",
        "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
        "        self.linear_h = nn.Linear(hidden_size,hidden_size)\n",
        "        self.linear_y = nn.Linear(hidden_size,1)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hprev):\n",
        "        \"\"\"\n",
        "        h = self.tanh(self.embedding(x) + self.linear_h(hprev))\n",
        "        y = self.linear_y(h)\n",
        "        \"\"\"\n",
        "        h = self.tanh(self.linear_x(x) + self.linear_h(hprev))\n",
        "        y = self.sigmoid(self.linear_y(h))\n",
        "        return h,y"
      ],
      "metadata": {
        "id": "SbssajRW_yHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model,inputs,targets,hprev,vocab_size):\n",
        "  loss_func = torch_energy_loss\n",
        "  seq_length = len(inputs)\n",
        "  outputs = []\n",
        "  for t in range(seq_length):\n",
        "    # For each character in the input sequence, pass through RNN with previous hidden state\n",
        "    hprev,y = model(torch.tensor([inputs[t]], device=device),hprev)\n",
        "    # Gradually build up matrix of output logits of size seq_length * vocab_size - we want it at every time step do not want this.\n",
        "    # Compute cross entropy loss for seq_length actual targets against estimated distributions\n",
        "    # RuntimeError: result type Float can't be cast to the desired output type Long\n",
        "    outputs.append(y)\n",
        "\n",
        "  outputs = torch.stack(outputs).squeeze(1)\n",
        "  loss = loss_func(targets, outputs)\n",
        "\n",
        "  # For truncated backprop, the next subsequence will use the final hidden state\n",
        "  # but will not backprop through it so we need to detach\n",
        "  hprev = hprev.detach()\n",
        "\n",
        "  return loss, hprev"
      ],
      "metadata": {
        "id": "Kblub6lCs1q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters to be logged by w and b.\n",
        "hidden_size = 10\n",
        "lr = 0.0001\n",
        "iterations = 10000\n",
        "input_parameters = ['TyreLife']\n",
        "dataset = 'v1'\n",
        "#vocab_size = int(max_tyre_life) + 1 # Possiblity for each tyre_life\n",
        "input_size = 1"
      ],
      "metadata": {
        "id": "ZEPgYoM5tVB7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PADDING\n",
        "def add_padding(max_race_size, sequence):\n",
        "    if len(sequence) >= max_race_size:\n",
        "        return sequence[:max_race_size]\n",
        "    else:\n",
        "        padding = [0] * (max_race_size - len(sequence))\n",
        "        return np.concatenate([sequence,padding])"
      ],
      "metadata": {
        "id": "kZnrmrWCZewQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy():\n",
        "  total, matches = 0, 0\n",
        "  no_pit_prediction_total, correct_no_pit_prediction = 0,0\n",
        "  pit_prediction_total, correct_pit_prediction = 0,0\n",
        "\n",
        "  for t in range(len(testing_inputs) - 1):\n",
        "      inputs = add_padding(int(max_race_size),testing_inputs[t])\n",
        "      outputs = add_padding(int(max_race_size),testing_outputs[t])\n",
        "      hprev = torch.zeros(hidden_size, device=device)\n",
        "\n",
        "      for x in range(len(inputs) - 1):\n",
        "\n",
        "        hprev, y = model(torch.tensor([inputs[x]], device=device, dtype=torch.float32), hprev)\n",
        "\n",
        "        if (y.item() > 0.5) == (outputs[x] == 1.0):\n",
        "          matches += 1\n",
        "\n",
        "        if (y.item() > 0.5):\n",
        "          pit_prediction_total += 1\n",
        "          if 1 in outputs[max(0, x - 2): min(int(max_race_size), x+4)]:\n",
        "              correct_pit_prediction += 1\n",
        "        else:\n",
        "          no_pit_prediction_total += 1\n",
        "          if y.floor().item() == outputs[x]:\n",
        "              correct_no_pit_prediction += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "  if total != 0:\n",
        "    print(f\"Total Accuracy: {matches / total * 100}\")\n",
        "  else:\n",
        "    print(\"Total Accuracy: 0.0\")\n",
        "\n",
        "  if no_pit_prediction_total != 0:\n",
        "    print(f\"Total No Pit Accuracy: {correct_no_pit_prediction / no_pit_prediction_total * 100}\")\n",
        "  else:\n",
        "    print(f\"Total No Pit Accuracy: 0.0\")\n",
        "\n",
        "  if pit_prediction_total != 0:\n",
        "    print(f\"Total Pit Accuracy: {correct_pit_prediction / pit_prediction_total * 100}\")\n",
        "  else:\n",
        "    print(f\"Total Pit Accuracy: 0.0\")\n",
        "\n",
        "  print(f\"Total Pit Predictions: {pit_prediction_total}\")"
      ],
      "metadata": {
        "id": "TmkbKIyMSOeA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNv1b(input_size,hidden_size)#vocab_size\n",
        "model.to(device)\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "n, p = 0, 0\n",
        "while n<=iterations:\n",
        "  hprev = torch.zeros(hidden_size, device=device) # reset RNN memory\n",
        "\n",
        "  # Why apply padding here\n",
        "  inputs = add_padding(int(max_race_size),training_inputs[p])\n",
        "  outputs = add_padding(int(max_race_size),training_outputs[p])\n",
        "\n",
        "  # Extract next subsequence of characters\n",
        "  inputs = torch.tensor(training_inputs[p],dtype=torch.float32, device=device)\n",
        "  targets = torch.tensor(training_outputs[p],dtype=torch.float32, device=device)\n",
        "\n",
        "  # Compute loss for current subsequence\n",
        "  loss, hprev = calculate_loss(model,inputs,targets,hprev,input_size)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if n % 500 == 0:\n",
        "    print('[{:}] Loss: {:.2f}'.format(n,loss.item()))\n",
        "    check_accuracy()\n",
        "\n",
        "  p += 1 # move data pointer\n",
        "  n += 1 # iteration counter\n",
        "\n",
        "  if p == int(max_race_size) - 1:\n",
        "    p = 0"
      ],
      "metadata": {
        "id": "NjMwOzL7uykr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cea483b-1fb2-4354-ec6f-3af87cec2fbe"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Loss: 1.18\n",
            "Total Accuracy: 94.24945763618533\n",
            "Total No Pit Accuracy: 97.2540958458612\n",
            "Total Pit Accuracy: 61.70616113744076\n",
            "Total Pit Predictions: 1055\n",
            "[500] Loss: 2.54\n",
            "Total Accuracy: 94.1603019406223\n",
            "Total No Pit Accuracy: 97.27188940092167\n",
            "Total Pit Accuracy: 63.2393084622384\n",
            "Total Pit Predictions: 1099\n",
            "[1000] Loss: 2.30\n",
            "Total Accuracy: 93.04288388956581\n",
            "Total No Pit Accuracy: 97.2400074594393\n",
            "Total Pit Accuracy: 47.52542372881356\n",
            "Total Pit Predictions: 1475\n",
            "[1500] Loss: 2.27\n",
            "Total Accuracy: 92.71597967250142\n",
            "Total No Pit Accuracy: 97.23053892215569\n",
            "Total Pit Accuracy: 44.22712933753943\n",
            "Total Pit Predictions: 1585\n",
            "[2000] Loss: 2.14\n",
            "Total Accuracy: 92.68328925079497\n",
            "Total No Pit Accuracy: 97.23253564631369\n",
            "Total Pit Accuracy: 47.49687108886108\n",
            "Total Pit Predictions: 1598\n",
            "[2500] Loss: 2.54\n",
            "Total Accuracy: 92.68328925079497\n",
            "Total No Pit Accuracy: 97.23253564631369\n",
            "Total Pit Accuracy: 47.80976220275344\n",
            "Total Pit Predictions: 1598\n",
            "[3000] Loss: 1.86\n",
            "Total Accuracy: 92.84079764628964\n",
            "Total No Pit Accuracy: 97.23710441066534\n",
            "Total Pit Accuracy: 46.990291262135926\n",
            "Total Pit Predictions: 1545\n",
            "[3500] Loss: 3.97\n",
            "Total Accuracy: 92.96561562007786\n",
            "Total No Pit Accuracy: 97.22014925373135\n",
            "Total Pit Accuracy: 45.5339153794493\n",
            "Total Pit Predictions: 1489\n",
            "[4000] Loss: 2.52\n",
            "Total Accuracy: 92.98344675919047\n",
            "Total No Pit Accuracy: 97.22066778586085\n",
            "Total Pit Accuracy: 45.51584625758597\n",
            "Total Pit Predictions: 1483\n",
            "[4500] Loss: 2.12\n",
            "Total Accuracy: 93.03694017652828\n",
            "Total No Pit Accuracy: 97.22222222222221\n",
            "Total Pit Accuracy: 45.32423208191126\n",
            "Total Pit Predictions: 1465\n",
            "[5000] Loss: 2.13\n",
            "Total Accuracy: 93.06071502867842\n",
            "Total No Pit Accuracy: 97.2229125248509\n",
            "Total Pit Accuracy: 45.2299245024022\n",
            "Total Pit Predictions: 1457\n",
            "[5500] Loss: 2.25\n",
            "Total Accuracy: 93.1082647329787\n",
            "Total No Pit Accuracy: 97.22429210134128\n",
            "Total Pit Accuracy: 45.38514920194309\n",
            "Total Pit Predictions: 1441\n",
            "[6000] Loss: 1.85\n",
            "Total Accuracy: 93.11718030253499\n",
            "Total No Pit Accuracy: 97.22455061935364\n",
            "Total Pit Accuracy: 45.41029207232267\n",
            "Total Pit Predictions: 1438\n",
            "[6500] Loss: 1.63\n",
            "Total Accuracy: 92.9715593331154\n",
            "Total No Pit Accuracy: 97.22032211927119\n",
            "Total Pit Accuracy: 47.54539340954943\n",
            "Total Pit Predictions: 1487\n",
            "[7000] Loss: 1.65\n",
            "Total Accuracy: 92.99236232874676\n",
            "Total No Pit Accuracy: 97.2209269793901\n",
            "Total Pit Accuracy: 47.7027027027027\n",
            "Total Pit Predictions: 1480\n",
            "[7500] Loss: 1.50\n",
            "Total Accuracy: 93.01910903741567\n",
            "Total No Pit Accuracy: 97.22170426999813\n",
            "Total Pit Accuracy: 47.99456152277362\n",
            "Total Pit Predictions: 1471\n",
            "[8000] Loss: 1.52\n",
            "Total Accuracy: 94.11572409284081\n",
            "Total No Pit Accuracy: 97.25320306018988\n",
            "Total Pit Accuracy: 64.06533575317604\n",
            "Total Pit Predictions: 1102\n",
            "[8500] Loss: 1.36\n",
            "Total Accuracy: 94.11869594935956\n",
            "Total No Pit Accuracy: 97.25328745237802\n",
            "Total Pit Accuracy: 64.03269754768392\n",
            "Total Pit Predictions: 1101\n",
            "[9000] Loss: 1.45\n",
            "Total Accuracy: 94.11869594935956\n",
            "Total No Pit Accuracy: 97.25328745237802\n",
            "Total Pit Accuracy: 64.03269754768392\n",
            "Total Pit Predictions: 1101\n",
            "[9500] Loss: 1.46\n",
            "Total Accuracy: 94.10680852328449\n",
            "Total No Pit Accuracy: 97.25294985250737\n",
            "Total Pit Accuracy: 64.16289592760181\n",
            "Total Pit Predictions: 1105\n",
            "[10000] Loss: 1.43\n",
            "Total Accuracy: 94.11869594935956\n",
            "Total No Pit Accuracy: 97.25328745237802\n",
            "Total Pit Accuracy: 64.03269754768392\n",
            "Total Pit Predictions: 1101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation here is that it decides to never pit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bKMUj6iCvzZy"
      }
    }
  ]
}