{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mistryvivek/YRKCS-PRBX/blob/main/PADL_Week_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUN-Fa5NwFKu"
      },
      "source": [
        "#Practical week 9: custom data loaders, a CNN from scratch and visualising loss curves\n",
        "\n",
        "The practical this week focusses on some low level practical issues that you will need to tackle in the assessment. We will see how to take an arbitrary dataset and write a custom data loader so that we can request random mini-batches during training like we did last week. Then you'll design and train your own CNN to learn a classification problem on the data. Working through this whole practical is great practice for the assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgQziDJTUVfX"
      },
      "source": [
        "##This week's dataset\n",
        "\n",
        "The task this week is to classify clothing images according to the type of garment. For this task, we're using the Fashion MNIST dataset. It's exactly the same format as the MNIST digits dataset we used last week (60k training images, 10k test images, 10 possible class labels, grayscale images of size $28\\times 28$). See the code below for descriptions of the 10 classes.\n",
        "\n",
        "Fashion MNIST is already set up as a dataset in PyTorch. However, we're going to pretend it's not so that you can experience writing your own custom data loader.\n",
        "\n",
        "The following code will download and uncompress the training and testing datasets, then load them as PyTorch tensors. You'll have to do the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuRPf33VVYxe"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "classes = ['T-shirt/top',\n",
        "           'Trouser',\n",
        "           'Pullover',\n",
        "           'Dress',\n",
        "           'Coat',\n",
        "           'Sandal',\n",
        "           'Shirt',\n",
        "           'Sneaker',\n",
        "           'Bag',\n",
        "           'Ankle boot']\n",
        "datafolder = 'fashion-mnist/'\n",
        "os.makedirs(datafolder,exist_ok=True)\n",
        "\n",
        "url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
        "files = ['train-images-idx3-ubyte','train-labels-idx1-ubyte','t10k-images-idx3-ubyte','t10k-labels-idx1-ubyte']\n",
        "for f in files:\n",
        "  datasets.utils.download_and_extract_archive(url+f+'.gz',datafolder)\n",
        "data_train = datasets.mnist.read_image_file(datafolder+files[0])\n",
        "labels_train = datasets.mnist.read_label_file(datafolder+files[1])\n",
        "data_test = datasets.mnist.read_image_file(datafolder+files[2])\n",
        "labels_test = datasets.mnist.read_label_file(datafolder+files[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HceJkXhcZYCD"
      },
      "source": [
        "The raw data is now loaded into tensors. Let's check we're happy with the shapes and contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Bv1kRpZjdr"
      },
      "source": [
        "print('TRAINING DATA')\n",
        "print(data_train.shape)\n",
        "print('Minimum value: {}'.format(data_train.min()))\n",
        "print('Maximum value: {}'.format(data_train.max()))\n",
        "print(data_train.dtype)\n",
        "print('TRAINING LABELS')\n",
        "print(labels_train.shape)\n",
        "print('Minimum value: {}'.format(labels_train.min()))\n",
        "print('Maximum value: {}'.format(labels_train.max()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dATUKl0qcaRU"
      },
      "source": [
        "Do the shapes of the two tensors make sense to you? One important note: we're storing the images currently as 8 bit integers to save space but we'll need to switch to floats for each mini-batch for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8A_H-T8ws0e"
      },
      "source": [
        "##Custom data loaders\n",
        "\n",
        "`torch.utils.data.Dataset` is an abstract class for representing a dataset. To write a custom data loader you need to inherit `Dataset` and override two methods:\n",
        "\n",
        "- `__len__` such that `len(dataset)` returns the number of elements in the dataset\n",
        "- `__getitem__` such that `dataset[i]` can be used to get the $i$th element from the dataset\n",
        "\n",
        "You can use the `__init__` constructor to set up your data. There are two common ways of constructing a data loader:\n",
        "\n",
        "1. Read the entire dataset into memory inside `__init__`. If the dataset isn't too big so that you can hold the whole thing in memory then this is a good idea since `__getitem__` won't need to read anything from disc. (This option is fine for the dataset in this practical).\n",
        "2. For larger datasets, `__init__` is just used to initialise where the dataset is stored and perhaps to read the labels into memory but no actual input data is loaded at this point. Then when `__getitem__` is called you actually load that specific item (for example an image) from disc.\n",
        "\n",
        "Anything that `__len__` or `__getitem__` need access to should be stored in `self`.\n",
        "\n",
        "**To do**:\n",
        "\n",
        "Create a custom dataset whose constructor takes as input a size $n\\times 28 \\times 28$ tensor of images and size $n$ tensor of class labels. `__getitem__` should return a single image of size $1\\times 28 \\times 28$ as a 32 bit float scaled between 0 and 1 (hint: `x.float()/255` converts `x` from 8 bit integers to 0..1 floats) and the corresponding class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqJzna7pan-s"
      },
      "source": [
        "class FashionMNISTDataset(Dataset):\n",
        "  \"\"\"Custom Fashion MNIST dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, data, labels):\n",
        "    # Your code goes here\n",
        "\n",
        "  def __len__(self):\n",
        "    # Your code goes here\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Your code goes here\n",
        "    return image, label\n",
        "\n",
        "train_data = FashionMNISTDataset(data_train,labels_train)\n",
        "test_data = FashionMNISTDataset(data_test,labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfkgwHfve4JV"
      },
      "source": [
        "Let's test whether your dataset is behaving as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqOI2eCmacx"
      },
      "source": [
        "image, label = train_data[0]\n",
        "print(image.shape) # should be 1 x 28 x 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzigoX5ae9tJ"
      },
      "source": [
        "Now we can create dataloaders from your two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQv81FQye0vR"
      },
      "source": [
        "batch_size=100\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8roOoNh7cn"
      },
      "source": [
        "Let's test them. Do these sizes look right?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm9ciKSbh9Ep"
      },
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7FnnIqafChY"
      },
      "source": [
        "##Visualising the dataset\n",
        "\n",
        "**To do**:\n",
        "\n",
        "Look at last week's practical and copy the code for visualing images from a mini-batch. Run it with your dataloader to inspect the data. Now modify it so that it prints class descriptions as given above rather than class IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M4VhTuef2rn"
      },
      "source": [
        "##Create a CNN\n",
        "\n",
        "**To do**:\n",
        "\n",
        "Now create a CNN to try to solve this classification problem. You could use the LeNet architecture from the last practical, the mini VGG I gave in my model solution or invent your own from scratch. Maybe you want to try batchnorm layers or even a residual block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWhOnFoePBDV"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Your code goes here...\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Your code goes here...\n",
        "\n",
        "# Instantiate the model - this initialises all weights and biases\n",
        "model = CNN()\n",
        "\n",
        "# Decide what loss function to use\n",
        "#loss_func = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS9bCmWLgJST"
      },
      "source": [
        "It's interesting to see how many learnable parameters your model has in total:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DsnOTQgy1_s"
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elXykErXiNCs"
      },
      "source": [
        "Let's put a mini-batch through your CNN, compute a loss and check everything looks ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzSlVUbIh28f"
      },
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "output = model(images)\n",
        "print(output.shape)\n",
        "loss = loss_func(output,labels)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SUbmmp6h04H"
      },
      "source": [
        "##Visualisation and training\n",
        "\n",
        "It's time to train your network. But this time we are going to visualise loss at every iteration and classification accuracy at every epoch for both the training and testing sets. Run the code below then execute the code cell underneath to plot the training curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwpxWh9bPQ3S"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "# Set up the optimiser\n",
        "optim = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "# Initialise some variables for computing and tracking stats\n",
        "iterations_per_epoch = math.ceil(len(train_data)/batch_size)\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "testing_losses = []\n",
        "testing_accuracies = []\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # One epoch on the training set\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  total_loss = 0\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output = model(inputs)\n",
        "    loss = loss_func(output,labels)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    pred_y = torch.argmax(output, 1)\n",
        "    correct += (pred_y == labels).sum()\n",
        "    total += float(labels.size(0))\n",
        "    total_loss += loss*inputs.shape[0]\n",
        "    if (i+1) % 100 == 0:\n",
        "       print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, iterations_per_epoch, loss.item()))\n",
        "  total_loss /= len(train_data)\n",
        "  training_losses.append(total_loss.item())\n",
        "  training_accuracies.append((correct/total).cpu())\n",
        "  print('Train accuracy over epoch {}: {:.4f}'.format(epoch+1,training_accuracies[-1]))\n",
        "\n",
        "  # One epoch on the test set\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  total_loss = 0\n",
        "  # Switch to evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      output = model(inputs)\n",
        "      loss = loss_func(output,labels)\n",
        "      pred_y = torch.argmax(output, 1)\n",
        "      correct += (pred_y == labels).sum()\n",
        "      total += float(labels.size(0))\n",
        "      total_loss += loss*inputs.shape[0]\n",
        "    test_accuracy = correct/total\n",
        "  total_loss /= len(test_data)\n",
        "  testing_losses.append(total_loss.item())\n",
        "  # Switch back to training mode\n",
        "  model.train()\n",
        "  testing_accuracies.append(test_accuracy.cpu())\n",
        "  print('Test accuracy at epoch {}: {:.4f}'.format(epoch+1,test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OWm40vzfdM3"
      },
      "source": [
        "plt.title(\"Training curve\")\n",
        "plt.plot(range(len(training_losses)),training_losses,'r')\n",
        "plt.plot(range(len(testing_losses)),testing_losses,'g')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Classification accuracy\")\n",
        "plt.plot(range(len(training_accuracies)),training_accuracies,'r')\n",
        "plt.plot(range(len(testing_accuracies)),testing_accuracies,'g')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFMpy-PjE6bS"
      },
      "source": [
        "What do you make of the plots? Has training converged yet? Is your model too powerful? (i.e. is overfitting happening?) Or not powerful enough? (Performance not improving much). Tweak your model or hyperparameters if you think there's a problem.\n",
        "\n",
        "**To do**:\n",
        "\n",
        "- Try plotting training loss at every iteration rather than just every epoch. Why does it look so different?\n",
        "- Reuse the visualisation code from last week to show example images along with their correct/estimate class.\n",
        "- You've probably noticed that training is starting to take a while. Try switching to using a GPU (see lecture slides for instructions).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement a ResNet (optional)\n",
        "\n",
        "Take any of the problems you've worked on over the past couple of weeks (e.g. MNIST digit or fashion classification) and try implementing your own resnet architecture to solve it. Usually, this would start with a convolutional layer to transform from the one input channel to some larger number of features then residual blocks which contain a number of convolutional layers applied in sequence. You might like to define a residual block as its own `nn.Module` then you can define your network using `nn.Sequential` including your new blocks. Instead of pooling, it is more common to use strided convolutions. Also, instead of fully connected layers at the end, it is more common to use global average pooling (i.e. pooling with kernel size equal to spatial dimensions). If your residual block changes the size of tensor either spatially or in number of channels, then you need to apply some kind of downsampling to the residual connection. How does your model compare to the other networks we tried?"
      ],
      "metadata": {
        "id": "vvznsQAhJc49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##An extra challenge: fully convolutional LeNet\n",
        "\n",
        "If you would like an extra challenge to work on, try this...\n",
        "\n",
        "As was pointed out in [this paper](https://arxiv.org/pdf/1605.06211.pdf), a CNN that includes fully connected layers can be thought of as having only convolution layers. The fully connected layers are just a special case where the filter size is $1\\times 1$. Start by training a modified version of LeNet in which you replace the fully connected (i.e. `torch.nn.Linear`) layers with convolutions. To do this you need to:\n",
        "\n",
        "1. Delete the flattening operation in the forward pass (that reshapes from $B\\times 16 \\times 5 \\times 5$ to $B\\times 400$).\n",
        "2. In the original LeNet, the second convolution uses no padding which shrinks the feature map by two pixels vertically and horizontally. This is undesirable for a fully convolutional network as it means regions at the boundary are treated differently. So, let's scrap that and use `padding=2` like in the first convolution layer.\n",
        "3. The first linear layer should be replaced by a convolution. Originally, the feature map at this point would have size $5\\times 5$ but because we just added padding, it will now have size $7\\times 7$. So, for this new convolution layer use kernel size $7\\times 7$ and no padding. We also want to only consider each region independently and not the overlaps between them so set `stride=7`. This is now exactly equivalent to flattening followed by a fully connected layer. It's a special case where the filter exactly spatially covers the input. The output size of this tensor will be $B\\times 120 \\times 1 \\times 1$, i.e. it has spatial size $1\\times 1$ and 120 channels.\n",
        "4. The second and third linear layers should be replaced with convolution layers with kernel size $1\\times 1$ and no padding and the appropriate number of input and output channels.\n",
        "5. The final output will now have size $B\\times 10 \\times 1 \\times 1$. Before passing to the loss function, you need to squeeze to size $B\\times 10$.\n",
        "\n",
        "You can now train the model exactly as before. However, once trained, you should be able to input images with size different to $28\\times 28$. The output will no longer have size $10\\times 1\\times 1$. For example, if you input an image of size $56\\times 56$, the output will be size $10\\times 2\\times 2$. At each of the four locations, you will get 10 values related to how likely it thinks each digit is at that location.\n",
        "\n",
        "Download the following image:\n",
        "\n",
        "![MNIST_grid2.png](data:image/png;base64,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)\n",
        "\n",
        "Upload it to your notebook's session storage and load it. The recommended way to read images is using `PIL.Image`. The torchvision transform `ToTensor` will convert this to a PyTorch tensor. E.g. see the following example:"
      ],
      "metadata": {
        "id": "Z7dwHggLDZ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "im = transform(Image.open('filename.png'))\n",
        "# im is now a C x H x W PyTorch tensor"
      ],
      "metadata": {
        "id": "hriKThkSxEuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the image through your trained network. Apply softmax to the output to convert to probabilities. Now visualise the output as $5\\times 5$ heat maps. For example, try visualising the first channel (representing the probability of a 0 digit). Do you see large probability where the zeros in the image are?"
      ],
      "metadata": {
        "id": "mdksx5VQxa_j"
      }
    }
  ]
}