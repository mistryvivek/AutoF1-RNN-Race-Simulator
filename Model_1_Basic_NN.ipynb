{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15snMOHrwpWO_qNjB-uqVjM_LwzWq-JuP",
      "authorship_tag": "ABX9TyPulEsIwRARqwAlFe8DPaBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mistryvivek/YRKCS-PRBX/blob/main/Model_1_Basic_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9BKFIobTm36L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self,vocab_size,hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding =  nn.Embedding(vocab_size, hidden_size)\n",
        "        self.linear_h = nn.Linear(hidden_size,hidden_size)\n",
        "        self.linear_y = nn.Linear(hidden_size,vocab_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self,x,hprev):\n",
        "        h = self.tanh(self.embedding(x) + self.linear_h(hprev))\n",
        "        y = self.linear_y(h)\n",
        "        return h,y"
      ],
      "metadata": {
        "id": "bWcnkSnqniIl"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function (Based upon wk11 practical)\n",
        "\n",
        "* Vocab size is not relevent here - going to give it binary classification values anyway.\n",
        "* Everything else can be applied for time series: use Cross Entropy Loss, Length of the race = seq_len.\n",
        "* For each lap, pass through the RNN with previous hidden state and build up hte matrix of outputs.\n",
        "* Detact the previous hidden state as a new one will be generated."
      ],
      "metadata": {
        "id": "S81x8lmwJ68-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model,inputs,targets,hprev):\n",
        "  loss_func = nn.BCEWithLogitsLoss()\n",
        "  seq_length = len(inputs)\n",
        "  outputs = torch.zeros(0,vocab_size)\n",
        "  for t in range(seq_length):\n",
        "    # For each character in the input sequence, pass through RNN with previous hidden state\n",
        "    hprev,y = model(torch.tensor([inputs[t]]),hprev)\n",
        "    # Gradually build up matrix of output logits of size seq_length * vocab_size\n",
        "    outputs = torch.cat((outputs,y))\n",
        "\n",
        "  # Compute cross entropy loss for seq_length actual targets against estimated distributions\n",
        "  loss = loss_func(outputs,torch.tensor(targets))\n",
        "\n",
        "  # For truncated backprop, the next subsequence will use the final hidden state\n",
        "  # but will not backprop through it so we need to detach\n",
        "  hprev = hprev.detach()\n",
        "\n",
        "  return loss, hprev"
      ],
      "metadata": {
        "id": "4b01se5zoVVe"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My dataset - how to format?\n",
        "\n",
        "* Training and testing\n",
        "* inputs_training/ inputs_testing/  outputs_testing/ output_training\n",
        "* For now, it will all just be a 2d array.\n",
        "\n",
        "ERROR:\n",
        "\n",
        "```2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
        "   2552\n",
        "   2553\n",
        "\n",
        "IndexError: index out of range in self```"
      ],
      "metadata": {
        "id": "icgJLYuiNA2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = r\"/content/drive/MyDrive/prbx_data/v1/\"\n",
        "max_race_size = 0"
      ],
      "metadata": {
        "id": "vjXV6o1kOB_-"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "training_inputs = []\n",
        "training_outputs = []\n",
        "\n",
        "RaceCalender22 = pd.read_csv(BASE_PATH + r\"2022/eventCalender2022.csv\")\n",
        "for _, row in RaceCalender22.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        # Load race data\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2022/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            # Filter data for the specific driver and sort by LapNumber\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            # Extract tyre life and stint columns\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "\n",
        "            # Create the stint change array\n",
        "            stint_change_array = []\n",
        "            for i in range(len(stint_array)):\n",
        "                if i == len(stint_array) - 1:  # Last lap\n",
        "                    stint_change_array.append(False)  # Treat as no change\n",
        "                else:\n",
        "                    stint_change_array.append(stint_array[i] != stint_array[i + 1])\n",
        "\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "\n",
        "            # Ensure tyre_life_array and stint_change_array are properly formatted\n",
        "            tyre_life_array = torch.tensor(tyre_life_array, dtype=torch.long)\n",
        "            stint_change_array = torch.tensor(stint_change_array, dtype=torch.long)\n",
        "            # Add to training data\n",
        "            training_inputs.append(tyre_life_array)\n",
        "            training_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "ub0zgeNtND3l"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "testing_inputs = []\n",
        "testing_outputs = []\n",
        "\n",
        "RaceCalender23 = pd.read_csv(BASE_PATH + r\"2023/eventCalender2023.csv\")\n",
        "for _, row in RaceCalender23.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        # Load race data\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2023/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            # Filter data for the specific driver and sort by LapNumber\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            # Extract tyre life and stint columns\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "\n",
        "            # Create the stint change array\n",
        "            stint_change_array = []\n",
        "            for i in range(len(stint_array)):\n",
        "                if i == len(stint_array) - 1:  # Last lap\n",
        "                    stint_change_array.append(False)  # Treat as no change\n",
        "                else:\n",
        "                    stint_change_array.append(stint_array[i] != stint_array[i + 1])\n",
        "\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "\n",
        "            # Ensure tyre_life_array and stint_change_array are properly formatted\n",
        "            tyre_life_array = torch.tensor(tyre_life_array, dtype=torch.long)\n",
        "            stint_change_array = torch.tensor(stint_change_array, dtype=torch.long)\n",
        "\n",
        "            # Add to testing data\n",
        "            testing_inputs.append(tyre_life_array)\n",
        "            testing_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "MQHvjvglTQNF"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "4015h9tIT5Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 1 # size of hidden layer of neurons\n",
        "hidden_size = 100\n",
        "vocab_size = int(max_race_size)\n",
        "lr = 0.001\n",
        "iterations = len(training_inputs) - 1"
      ],
      "metadata": {
        "id": "s-1yQVYSUMBf"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(output_size,hidden_size)\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "# Need to initalise to handle initial logic.\n",
        "n, p = 0, 0\n",
        "# We assume that these are the same length.\n",
        "current_training_sequence = training_inputs[0]\n",
        "current_target_sequence = training_outputs[0]\n",
        "# Difficulties with sequence len not being fixed.\n",
        "seq_length = len(training_inputs[0])\n",
        "while n<=iterations:\n",
        "  # One iteration of truncated backdrop\n",
        "  if p >= len(current_training_sequence) - 1:\n",
        "    p=0\n",
        "    hprev = torch.zeros(hidden_size,) # reset RNN memory\n",
        "    n +=1\n",
        "    current_training_sequence = training_inputs[n]\n",
        "    current_target_sequence = training_outputs[n]\n",
        "  inputs = current_training_sequence[:p + 1]\n",
        "  targets = current_target_sequence[:p + 1]\n",
        "\n",
        "  # Compute loss for current subsequence\n",
        "  loss, hprev = calculate_loss(model,inputs,targets,hprev)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if n % 500 == 0:\n",
        "    print('[{:}] Loss: {:.2f}'.format(n,loss.item()))\n",
        "\n",
        "  p += 1 # move data pointer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oxZaedx7T8Ch",
        "outputId": "3cfb35e3-04bf-48e6-f996-c92d211215e5"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-52ce68614e1b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Compute loss for current subsequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-173-a3150f7677d0>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(model, inputs, targets, hprev)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# For each character in the input sequence, pass through RNN with previous hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Gradually build up matrix of output logits of size seq_length * vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-b7de3db67748>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hprev)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IndexError: index out of range in self\n",
        "\n",
        "* Double check my input data\n",
        "\n"
      ],
      "metadata": {
        "id": "NeZ8MvJwW_4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_inputs) == len(training_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i2tG_ZxXEmu",
        "outputId": "c6d3746a-f948-4a11-b179-3b4de05cdab6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_inputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT6aLRA5dPL9",
        "outputId": "e8e6d4d3-1de1-4d5f-8dbe-0d95a47be447"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_outputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuompqxedSuP",
        "outputId": "a6d1faa0-18e4-4ec1-cf7f-bac2c0979302"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}