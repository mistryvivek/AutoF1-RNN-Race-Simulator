{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1inTk-v8H-YV0f-RB43C0Z6yEVWKAG7Sl",
      "authorship_tag": "ABX9TyNIfiNg8b9F66MP7FkXEqLS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mistryvivek/YRKCS-PRBX/blob/main/Model_2_Classifer_NN_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "OPEp33YxUEHK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "BASE_PATH = r\"/content/drive/MyDrive/prbx_data/v1/\"\n",
        "max_race_size = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71yTDkeVUERu",
        "outputId": "0a5b7192-8df5-45a8-e256-e567bdf590eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change Model One to a multi-class classifier\n",
        "\n",
        "* No pit or a tire type.\n",
        "* 0: NO PIT, 1: SOFT, 2: MEDIUM, 3: HARD, 4: INTERS, 5: WETS"
      ],
      "metadata": {
        "id": "VEnSVchiUGQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "training_inputs = []\n",
        "training_outputs = []\n",
        "\n",
        "# Encoding tires type\n",
        "tires = {\"SOFT\": 1, \"MEDIUM\": 2, \"HARD\": 3, \"INTERMEDIATE\": 4, \"WET\": 5}\n",
        "\n",
        "\n",
        "RaceCalender22 = pd.read_csv(BASE_PATH + r\"2022/eventCalender2022.csv\")\n",
        "for _, row in RaceCalender22.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2022/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            tires_used = []\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "            compound_array = TempRaceLoadDriver['Compound'].values\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            stint_change_array = [0 if stint_array[i] == stint_array[i + 1] else tires[compound_array[i+1]]\n",
        "                                  for i in range(len(stint_array) - 1)] + [0]\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "\n",
        "            mandatory_change_made = [1 if stint_array[i] == 1 else 0\n",
        "                                     for i in range(len(stint_array))]\n",
        "            mandatory_change_made = np.array(mandatory_change_made)\n",
        "\n",
        "            combined_array = np.stack([stint_change_array, mandatory_change_made], axis=1)\n",
        "\n",
        "            training_inputs.append(combined_array)\n",
        "            training_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "FzAK3i7pUJOm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "testing_inputs = []\n",
        "testing_outputs = []\n",
        "\n",
        "# Encoding tires type\n",
        "tires = {\"SOFT\": 1, \"MEDIUM\": 2, \"HARD\": 3, \"INTERMEDIATE\": 4, \"WET\": 5}\n",
        "\n",
        "RaceCalender23 = pd.read_csv(BASE_PATH + r\"2023/eventCalender2023.csv\")\n",
        "for _, row in RaceCalender23.iterrows():\n",
        "    if row['EventFormat'] != 'testing':\n",
        "        TempRaceLoad = pd.read_csv(\n",
        "            BASE_PATH + f\"2023/{row['RoundNumber']}_{row['OfficialEventName']}/{row['RoundNumber']}_{row['OfficialEventName']}_Race.csv\".replace(\" \", \"_\")\n",
        "        )\n",
        "        for driver in TempRaceLoad['Driver'].unique():\n",
        "            TempRaceLoadDriver = TempRaceLoad[TempRaceLoad['Driver'] == driver].sort_values(by='LapNumber', ascending=True)\n",
        "            tyre_life_array = TempRaceLoadDriver['TyreLife'].values\n",
        "            stint_array = TempRaceLoadDriver['Stint'].values\n",
        "            compound_array = TempRaceLoadDriver['Compound'].values\n",
        "\n",
        "            if max(TempRaceLoadDriver['LapNumber']) > max_race_size:\n",
        "                max_race_size = max(TempRaceLoadDriver['LapNumber'])\n",
        "\n",
        "            stint_change_array = [0 if stint_array[i] == stint_array[i + 1] else tires[compound_array[i+1]]\n",
        "                                  for i in range(len(stint_array) - 1)] + [0]\n",
        "            stint_change_array = np.array(stint_change_array)\n",
        "            mandatory_change_made = [1 if stint_array[i] == 1 else 0\n",
        "                                     for i in range(len(stint_array))]\n",
        "            mandatory_change_made = np.array(mandatory_change_made)\n",
        "\n",
        "            combined_array = np.stack([stint_change_array, mandatory_change_made], axis=1)\n",
        "\n",
        "            testing_inputs.append(combined_array)\n",
        "            testing_outputs.append(stint_change_array)"
      ],
      "metadata": {
        "id": "J5-GG1q8UOaq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNv1b(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size): #vocab_size\n",
        "        super(RNNv1b, self).__init__()\n",
        "        #self.embedding =  nn.Embedding(vocab_size, hidden_size)\n",
        "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
        "        self.linear_h = nn.Linear(hidden_size,hidden_size)\n",
        "        self.linear_y = nn.Linear(hidden_size,1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self,x,hprev):\n",
        "        \"\"\"\n",
        "        h = self.tanh(self.embedding(x) + self.linear_h(hprev))\n",
        "        y = self.linear_y(h)\n",
        "        \"\"\"\n",
        "        h = self.tanh(self.linear_x(x) + self.linear_h(hprev))\n",
        "        y = self.linear_y(h)\n",
        "        return h,y"
      ],
      "metadata": {
        "id": "5S8M5a4BUnR0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model,inputs,targets,hprev,vocab_size):\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  seq_length = len(inputs)\n",
        "  outputs = []\n",
        "  for t in range(seq_length):\n",
        "    # For each character in the input sequence, pass through RNN with previous hidden state\n",
        "    hprev,y = model(torch.tensor(inputs[t], device=device),hprev)\n",
        "    # Gradually build up matrix of output logits of size seq_length * vocab_size - we want it at every time step do not want this.\n",
        "    # Compute cross entropy loss for seq_length actual targets against estimated distributions\n",
        "    # RuntimeError: result type Float can't be cast to the desired output type Long\n",
        "    outputs.append(y)\n",
        "\n",
        "  outputs = torch.stack(outputs).squeeze(1)\n",
        "  loss = loss_func(outputs ,targets)\n",
        "\n",
        "  # For truncated backprop, the next subsequence will use the final hidden state\n",
        "  # but will not backprop through it so we need to detach\n",
        "  hprev = hprev.detach()\n",
        "\n",
        "  return loss, hprev"
      ],
      "metadata": {
        "id": "XJY7yPTpU1py"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}